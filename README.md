# interpretability_for_adversarial_detection
Work exploring the use of interpretability techniques such as saliency maps to help detect machine learning adversarial attacks

After installing module requirements, place the files in 'foolbox_replacement_files/models' with those in 'foolbox/models' in your site-packages directory. (Due to this, the use of a virtual enviroment is reccomended)

